---
title: "CS 670: Data Science"
subtitle: "Network Traffic Analysis for Intrusion Detection"
author: "Ranjitha Rani"
format: html
---

## Introduction

The rise in global cyber threats has heightened the importance of intrusion detection systems in modern network security. With organizations generating massive volumes of network traffic data, developing effective data-driven intrusion detection strategies has become increasingly critical. The UNSW-NB15 dataset is one such comprehensive resource that simulates real-world network traffic, including both normal activity and a wide range of synthetic attacks. 



## 1. Data Selection and Overview

**Source:** Australian Centre for Cyber Security (ACCS), UNSW Canberra  
**Dataset:** UNSW-NB15 Public Dataset

**Creator & Purpose:**  
The dataset was generated by the IXIA PerfectStorm tool in 2015, simulating normal user behavior along with a variety of modern synthetic attack types. It includes raw traffic features and labelled records for supervised learning and analysis of cyber-attacks.

**Attributes & Description:**  
The dataset includes over 2 million network flow records distributed over four CSV files. Each record contains 49 attributes representing:

- **Basic connection features**  
  Source/destination IP, port, protocol

- **Flow features**  
  Number of packets, bytes, time duration, direction

- **Content features**  
  Login attempts, HTTP methods, service requests

- **Labelled fields**  
  `label` (binary), `attack_cat` (multiclass attack type)


**Challenges and Considerations:**  
> Some features like `is_ftp_login` and `ct_ftp_cmd` require special handling.  
> Mixed data types and sparsely populated fields posed representation challenges.  


This dataset serves as a strong foundation for detailed exploratory analysis, interactive visualizations, and classification modeling to detect network intrusions.


## Import Libraries {.unnumbered}

This section loads all essential libraries used for preprocessing, modeling (classification & regression), interactive visualizations, and evaluation.  

```{python}
#| code-fold: true
#| code-summary: "Click to show/hide library imports"


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import plotly.express as px
import plotly.graph_objects as go
import statsmodels.api as sm
from scipy import stats
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import GradientBoostingRegressor
from xgboost import XGBRegressor
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,
    confusion_matrix, classification_report, average_precision_score,
    precision_recall_curve, roc_curve
)
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from pandas.plotting import scatter_matrix
import warnings
warnings.filterwarnings("ignore")
```

## 2. Data Preprocessing

> The dataset is available in four csv files.

### 2.1 Merging Dataset using features.csv

The dataset is divided into four .csv files.
I have combined all four of them using the features.csv files for the column attributes and using it for further steps.

```{python}
#| code-fold: true
#| code-summary: "Show Code"


df1 = pd.read_csv("UNSW-NB15_1.csv", encoding="ISO-8859-1")
df2 = pd.read_csv("UNSW-NB15_2.csv", encoding="ISO-8859-1")
df3 = pd.read_csv("UNSW-NB15_3.csv", encoding="ISO-8859-1")
df4 = pd.read_csv("UNSW-NB15_4.csv", encoding="ISO-8859-1")
df_features = pd.read_csv("NUSW-NB15_features.csv", encoding="ISO-8859-1")


df1.columns = df_features['Name']
df2.columns = df_features['Name']
df3.columns = df_features['Name']
df4.columns = df_features['Name']

print('Features:')
display(df_features.head())

merged_df=pd.concat([df1,df2,df3,df4],ignore_index=True)
print('Merged Dataset:')
display(merged_df.head())
print("dataset shape: ",merged_df.shape)

```
### 2.2 Identifying Numerical and Categorical Columns

Classifying all the numerical and categorical columns in the dataset for a broad overview of features involved.
```{python}
#| code-fold: true
#| code-summary: "Show Code"

numerical_columns = merged_df.select_dtypes(include=['number']).columns
categorical_columns = merged_df.select_dtypes(exclude=['number']).columns

print(f"There are {len(numerical_columns)} Numerical Columns in the dataset:")
print(list(numerical_columns)) 

print(f"\nThere are {len(categorical_columns)} Categorical Columns in the dataset:")
print(list(categorical_columns)) 

```

### 2.3 Duplicate Handling

Duplicates in the dataset pose a high chance of redundancy and thus affect modelling.

```{python}
#| code-fold: true
#| code-summary: "Show Code"


duplicate_count = merged_df.duplicated().sum()
print(f"Duplicates in dataset: {duplicate_count}")


total_rows_before = merged_df.shape[0]


merged_df = merged_df.drop_duplicates()

total_rows_after = merged_df.shape[0]
print(f"New dataset shape after removing duplicates: {merged_df.shape}")



```

The dataset has been filtered out of duplicates.

### 2.4 Missing Data Handling

Certain columns in the dataset have missing data.
```{python}
#| code-fold: true
#| code-summary: "Show Code"

missing_data = merged_df.isnull().sum().reset_index()
missing_data.columns = ["Column Name", "Total Missing Values"]
missing_data["% Missing"] = (missing_data["Total Missing Values"] / len(merged_df) * 100).round(2)


missing_data = missing_data[missing_data["Total Missing Values"] > 0].sort_values(by="% Missing", ascending=False)


missing_data
```

The columns having major missing values include: attack_cat; is_ftp_login, ct_flw_http_mthd.



### 2.4.1 Handling 'attack_cat'

attack_cat: The name of each attack category. In this data set , nine categories e.g. Fuzzers, Analysis, Backdoors, DoS Exploits, Generic, Reconnaissance, Shellcode and Worms

To ensure consistency :

-**Handled Missing Values** – Assigned "unknown_attack" to missing attack labels where label = 1 and "normal" otherwise.

-**Standardized Attack Names** – Trimmed spaces, converted to lowercase, and renamed ambiguous categories (e.g., "dos" → "denial_of_service").

-**Encoded Categories**– Converted attack labels into numerical values using Label Encoding for machine learning compatibility.


```{python}
#| code-fold: true
#| code-summary: "Show code"

if 'label' in merged_df.columns:
    merged_df.loc[(merged_df['attack_cat'].isna()) & (merged_df['label'] == 1), 'attack_cat'] = 'unknown_attack'

merged_df['attack_cat'].fillna('normal', inplace=True)

merged_df['attack_cat'] = merged_df['attack_cat'].str.strip().str.lower()

attack_mapping = {
    'dos': 'denial_of_service',
    'ddos': 'denial_of_service',
    'worm': 'worm_attack',
    'backdoor': 'backdoor_attack',
    'recon': 'reconnaissance',
    'fuzzers': 'fuzzing_attack',
}


merged_df['attack_cat'] = merged_df['attack_cat'].replace(attack_mapping)


le = LabelEncoder()
merged_df['attack_cat_encoded'] = le.fit_transform(merged_df['attack_cat'])


print("Unique values in attack_cat after processing:")
print(merged_df['attack_cat'].value_counts())

print("\nEncoded values:")
print(merged_df[['attack_cat', 'attack_cat_encoded']].drop_duplicates())


```

### 2.4.2 'Handling ct_flw_http_mthd'

ct_flw_http_mthd: Number of flows that has methods such as Get and Post in http service.

To ensure data consistency, we impute missing values in ct_flw_http_mthd (HTTP flow methods) using a data-driven approach:

**Identified Mode & Median** The most frequent (mode) and middle value (median) were computed.
**Smart Imputation**If the mode is 0 and appears frequently, we assign 0; otherwise, we use the median to fill missing values.
**-**This method balances preserving common trends while preventing bias from extreme values.

```{python}
#| code-fold: true
#| code-summary: "Show Code"

http_mode = merged_df['ct_flw_http_mthd'].mode()[0]


http_median = merged_df['ct_flw_http_mthd'].median()


fill_value = 0 if http_mode == 0 else http_median


merged_df['ct_flw_http_mthd'].fillna(fill_value, inplace=True)


print(f"Filling missing values with: {fill_value}")
print(merged_df['ct_flw_http_mthd'].describe())
```

### 2.4.3 'Handling is_ftp_login'

is_ftp_login: If the ftp session is accessed by user and a password then it is - 1 or else is 0.

**To maintain accuracy while filling missing values in is_ftp_login** :

- Group by Protocol (proto): Since FTP-related logins depend on protocol type, missing values are filled using the most frequent (mode) value within each protocol group.
- If no mode exists for a group, we assign 0 (assuming no FTP login).
- The column remains in integer format for model compatibility.

```{python}
#| code-fold: true
#| code-summary: "Show Code"


merged_df['is_ftp_login'] = merged_df['is_ftp_login'].fillna(
    merged_df.groupby('proto')['is_ftp_login'].transform(lambda x: x.mode()[0] if not x.mode().empty else 0)
)

def convert_to_binary(df, column):
    df[column] = df[column].fillna(0)  
    df[column] = (df[column] > 0).astype(int)
    return df

convert_to_binary(merged_df, 'is_ftp_login')

print(merged_df['is_ftp_login'].value_counts())


```

### 2.4.4 'Handling ct_ftp_cmd'


ct_ftp_cmd: Number of flows that has a command in ftp session.

Some rows in the `ct_ftp_cmd` column contained empty strings instead of numeric values. To ensure consistency and avoid errors during modeling, replace blank entries with `'0'` and convert the column to integer type.

```{python}
#| code-fold: true
#| code-summary: "Show Code"


merged_df['ct_ftp_cmd'] = merged_df['ct_ftp_cmd'].replace(' ', '0').fillna(0).astype(int)
```

### 2.4.5 'Handling sport and dsport'

sport: source port number
dsport: destination port number

Checking and ensuring all the port numbers to be of integer type so that it does not cause any interruptions with data analysis.

```{python}
merged_df['sport'] = pd.to_numeric(merged_df['sport'], errors='coerce').astype('Int64')
merged_df['dsport'] = pd.to_numeric(merged_df['dsport'], errors='coerce').astype('Int64')
```

## 3. Exploratory Data analysis

### 3.1 Attack Categorization

Before modeling, it's essential to understand the distribution of attack categories in the dataset. This chart visualizes how network traffic instances are spread across different attack types in the UNSW-NB15 dataset.

This step is important because:

**-**It helps identify class imbalance, which can significantly affect the performance of classification models.

**-**It highlights which types of attacks are most and least represented, guiding feature engineering, model selection, and evaluation strategies.

```{python}
#| code-fold: true
#| code-summary: "Show Code"

if "attack_cat" in merged_df.columns:
    attack_counts = merged_df["attack_cat"].value_counts().reset_index()
    attack_counts.columns = ["Attack Category", "Count"]

    
    fig = px.pie(
        attack_counts,
        names="Attack Category",
        values="Count",
        title="Attack Categories Distribution",
        hole=0.4, 
        color="Attack Category"
    )

    fig.show()
else:
    print("Column 'attack_cat' not found in the dataset!")
```

**Inference**: 
From the chart, it's clear that the dataset is highly imbalanced, with "normal" traffic making up 95.2% of the total data. All attack categories combined represent less than 5% of the data, with some categories like worms, backdoors, and shellcode appearing extremely infrequently (less than 0.1%).

This imbalance poses several challenges:

**-**Models may bias toward predicting “normal” traffic to achieve high overall accuracy.

**-**Rare attacks may be overlooked, resulting in poor recall for minority classes — which is unacceptable in intrusion detection.

**-**It may require sampling techniques (e.g., SMOTE, undersampling), or cost-sensitive modeling to improve performance on minority classes.



### 3.2 Protocol Usage Across Attacks

Analysing how frequently each network protocol appears in the dataset, aggregated across all attack types.

This visualization is important because:

**-**It helps identify which protocols are most common in both normal and attack traffic.

**-**It highlights protocol-based bias in the dataset — models may overfit to common protocols.

**-**It informs feature importance — if a protocol is strongly correlated with attacks or normal traffic, it may be a valuable predictive feature.

```{python}
#| code-fold: true
#| code-summary: "Show Code"

if "proto" in merged_df.columns and "attack_cat" in merged_df.columns:
  
    proto_counts = merged_df.groupby("proto")["attack_cat"].count().reset_index()
    proto_counts.columns = ["Protocol", "Count"]

   
    fig = px.bar(
        proto_counts,
        x="Protocol",
        y="Count",
        text="Count",
        title="Protocol Usage Across Attacks",
        labels={"Protocol": "Network Protocol", "Count": "Number of Occurrences"},
        color="Count",
        color_continuous_scale="Teal",
    )

 
    fig.update_traces(texttemplate='%{text}', textposition='outside')
    fig.update_layout(xaxis_tickangle=-45, height=600)

   
    fig.show()
else:
    print("Columns 'proto' or 'attack_cat' not found in the dataset!")

```

**Inference**: 
From the plot, we can see that TCP, UDP, and ICMP dominate the dataset. Specifically:

- TCP appears more than 1.4 million times, indicating it's the most common protocol involved in both normal and attack traffic.

- UDP and ICMP follow, with substantial counts.

- All other protocols (e.g., EIGRP, GRE, IPIP, etc.) have very low frequencies — some are almost negligible.


### 3.3 Source vs Destination Port Mapping

This scatter plot maps source ports to destination ports across network flows to visualize traffic patterns. Each dot represents a connection, and color intensity reflects destination port values.

This visualization is important because:

**-**It helps identify common port pairs used in the traffic — particularly patterns associated with attack behavior.

**-**Certain ports (e.g., 0, 80, 443) are often associated with specific services or vulnerabilities.



```{python}
#| code-fold: true
#| code-summary: "Show Code"

if "sport" in merged_df.columns and "dsport" in merged_df.columns:
    import plotly.express as px

    
    top_sport = merged_df["sport"].value_counts().nlargest(15).reset_index()
    top_sport.columns = ["Source Port", "Count"]


    
    fig3 = px.scatter(
        merged_df.sample(5000),  
        x="sport",
        y="dsport",
        title="Source vs Destination Ports Mapping",
        labels={"sport": "Source Port", "dsport": "Destination Port"},
        color="dsport",
        color_continuous_scale="Viridis",
        opacity=0.6,
    )
    fig3.update_layout(height=600)



    fig3.show()

else:
    print("Columns 'sport' or 'dsport' not found in the dataset!")
```

**Inference**: 
We observe a high concentration of traffic around lower destination port ranges (0–1024) — this aligns with well-known service ports like HTTP (80), HTTPS (443), FTP (21), etc.Some traffic shows destination ports in the higher dynamic/private range (49152–65535), often associated with ephemeral ports and less structured traffic patterns.There's a visible diagonal streak, indicating some port mirroring behavior, where the same port is used on both ends — this might be normal in certain applications but could also point to scripted behavior or replayed packets.



### 3.4 Correlation Heatmap

The heatmap visualizes the pairwise correlation between all numeric features in the dataset. It helps with:

**-**Reducing redundancy: Highly correlated features may carry similar information and can be removed to simplify models.

**-**Avoiding multicollinearity: In models like logistic regression, multicollinearity can distort the interpretation of coefficients.

**-**Identifying strong predictors: Features highly correlated with the target (e.g., Label or attack_cat_encoded) may be useful for classification.

```{python}

#| code-fold: true
#| code-summary: "Show Code"




numeric_df = merged_df.select_dtypes(include=['int64', 'float64'])


corr_matrix = numeric_df.corr()


plt.figure(figsize=(16, 12))
sns.heatmap(corr_matrix, cmap='coolwarm', linewidths=0.5)
plt.title("Feature Correlation Heatmap")
plt.show()


```
**Inference**: 
Many flow-related features like Spkts, Dpkts, Stime, Ltime, and tcprtt show strong positive correlation with each other. This suggests that these may be redundant and one or two could represent the group.There is a clear block of high correlation among features, such as ct_srv_src, ct_srv_dst, ct_dst_ltm, etc. These are connection tracking features, and their collinearity implies similar behavior tracking logic.Features like ct_state_ttl, ct_dst_sport_ltm, and ct_src_ltm show noticeable correlation with the encoded attack label, hinting at their potential predictive power.Some features such as is_ftp_login and ct_flw_http_mthd appear relatively uncorrelated with most others, meaning they may offer unique information.


### 3.5 Pairplots

The pairplot displays the relationships between selected features and their distribution across different attack categories, using color-coded encoding.

```{python}
#| code-fold: true
#| code-summary: "Show Code"

sampled_df = merged_df.sample(n=25000, random_state=43)

selected_features = ['ct_dst_sport_ltm', 'ct_src_ ltm', 'ct_state_ttl', 'ct_ftp_cmd', 'ct_flw_http_mthd', 'attack_cat_encoded']


sns.pairplot(sampled_df[selected_features], hue='attack_cat_encoded', palette='deep', corner=True)
plt.suptitle("Pairplot of Selected Features by Attack Category", y=1.02)
plt.show()
```

**Inference**:

- Certain features like ct_state_ttl and ct_dst_sport_ltm show clear class separation, where some categories occupy distinct bands or regions — indicating high predictive potential.

- ct_flw_http_mthd and ct_ftp_cmd are sparse but distinct for a few attack categories, possibly linked to specific protocol misuse (e.g., HTTP-based fuzzing or FTP-based backdoors).


### 3.6 Feature Distribution - Numerical 

Frequency distribution of a numerical feature using a histogram, overlaid with a Kernel Density Estimate (KDE) curve.
The histogram represents the count of data points within specific intervals (bins).
The KDE curve provides a smoothed estimate of the feature's probability density, helping to visualize the overall shape of the distribution.

This is important as it:

**-**It provides insight into data skewness and variability, which affect scaling and model assumptions.

**-**Identifies outliers or extreme values that may distort learning.

**-**Highlights the need for normalization or transformation before feeding features into certain models (like Logistic Regression or KNN).


```{python}
#| code-fold: true
#| code-summary: "Show Code"


sampled_df = merged_df.sample(n=20000, random_state=43)


dist_features = ['dur', 'sbytes', 'dbytes', 'sttl', 'dttl', 'sloss']


for feature in dist_features:
    plt.figure(figsize=(8, 4))
    sns.histplot(sampled_df[feature], kde=True, bins=30, color='lightcoral')
    plt.title(f"Distribution of '{feature}'")
    plt.xlabel(feature)
    plt.ylabel("Frequency")
    plt.tight_layout()
    plt.show()
```

**Inference**: 
Highly Skewed Distributions:dur, sbytes, and dbytes show heavy right skew — most flows are short and small, with a few large ones. These outliers could affect model performance and should be handled (e.g., via log-scaling or clipping).

### 3.7 Outliers

This boxplot is used to identify outliers across all numerical columns in the dataset. 

```{python}
#| code-fold: true
#| code-summary: "Show Code"

plt.figure(figsize=(15, 6))
merged_df[numerical_columns].boxplot(rot=90)
plt.title("Boxplot of Numerical Columns (Checking Outliers)")
plt.savefig("Boxplot of Numerical Columns (Checking Outliers).png")
plt.show()
```

**Inference**: 


- sload, dload, dbytes, and sbytes have extreme high values, possibly due to long flows or large data transfers.

- stcpb, dtcpb also show large spread and high-value outliers.

- Most other features have compact distributions, especially binary or categorical ones (e.g., is_ftp_login, ct_flw_http_mthd, etc.).

### 3.8 Statistical Analysis

Performing summary statistics and outlier mitigation for all numerical features. Specifically,calculating descriptive statistics (mean, median, standard deviation, etc.) and use the interquartile range (IQR) method to detect and handle outliers.

This is important because:

**-**Outliers can distort model performance — especially in distance-based algorithms like KNN or models sensitive to variance.

**-**Replacing outliers with median values helps maintain the general distribution of the data without introducing extreme bias.

**-**Generating summary statistics gives a quick overview of the spread, central tendency, and scale differences across features — which is useful for choosing scaling techniques later.



```{python}
#| code-fold: true
#| code-summary: "Show Code"

numerical_columns = merged_df.select_dtypes(include=['float64', 'int64']).columns.tolist()


exclude_columns = ['sport', 'swim', 'dwim', 'stcpb', 'dtcpb', 'Stime', 'Ltime']


numerical_columns = [col for col in numerical_columns if col not in exclude_columns]


for col in numerical_columns:
    median_value = merged_df[col].median()  # Getting the median value
    lower_bound = merged_df[col].quantile(0.25) - 1.5 * (merged_df[col].quantile(0.75) - merged_df[col].quantile(0.25))  # Lower bound for outliers
    upper_bound = merged_df[col].quantile(0.75) + 1.5 * (merged_df[col].quantile(0.75) - merged_df[col].quantile(0.25))  # Upper bound for outliers

    # Replace outliers with the median value
    merged_df[col] = merged_df[col].apply(lambda x: median_value if x < lower_bound or x > upper_bound else x)


summary_stats = merged_df[numerical_columns].describe().transpose()


display(summary_stats)
```



## 4. Feature Engineering and selection

Engineering the numerical values for selecting the best features for the modelling.

In addition to selecting existing features, engineering new features from raw network flow data to capture more meaningful patterns for classification. The derived features include ratios, aggregates, and interactions between flow-level statistics like bytes, packets, jitter, and TCP setup times.



**Generating following features**:

- Duration: Difference between Ltime and Stime

- Ratios: Capture asymmetry between source and destination metrics (e.g., byte ratio, jitter ratio)

- Aggregates: Total counts or sums across both directions (e.g., total packets)

- Interaction Terms: Multiplicative interactions between features to capture intensity of communication

- Statistical Differences: Difference in TCP sequence numbers or average packet sizes
```{python}
#| code-fold: true
#| code-summary: "Show Code"

def generate_features(df):
    # Duration
    df['duration'] = df['Ltime'] - df['Stime']

    # Ratios
    df['byte_ratio'] = df['sbytes'] / (df['dbytes'] + 1)
    df['pkt_ratio'] = df['Spkts'] / (df['Dpkts'] + 1)
    df['load_ratio'] = df['Sload'] / (df['Dload'] + 1)
    df['jit_ratio'] = df['Sjit'] / (df['Djit'] + 1)
    df['inter_pkt_ratio'] = df['Sintpkt'] / (df['Dintpkt'] + 1)
    df['tcp_setup_ratio'] = df['tcprtt'] / (df['synack'] + df['ackdat'] + 1)

    # Aggregate Features
    df['total_bytes'] = df['sbytes'] + df['dbytes']
    df['total_pkts'] = df['Spkts'] + df['Dpkts']
    df['total_load'] = df['Sload'] + df['Dload']
    df['total_jitter'] = df['Sjit'] + df['Djit']
    df['total_inter_pkt'] = df['Sintpkt'] + df['Dintpkt']
    df['total_tcp_setup'] = df['tcprtt'] + df['synack'] + df['ackdat']

    # Interaction Features
    df['byte_pkt_interaction_src'] = df['sbytes'] * df['Spkts']
    df['byte_pkt_interaction_dst'] = df['dbytes'] * df['Dpkts']
    df['load_jit_interaction_src'] = df['Sload'] * df['Sjit']
    df['load_jit_interaction_dst'] = df['Dload'] * df['Djit']
    df['pkt_jit_interaction_src'] = df['Spkts'] * df['Sjit']
    df['pkt_jit_interaction_dst'] = df['Dpkts'] * df['Djit']

    # Statistical Features
    df['mean_pkt_size'] = df['smeansz'] + df['dmeansz']
    df['tcp_seq_diff'] = df['stcpb'] - df['dtcpb']

    return df


generate_features(merged_df)


columns_to_drop = ['sport', 'dsport', 'proto','srcip', 'dstip','state', 'service']
merged_df.drop(columns=columns_to_drop, inplace=True)
```

**Inference**:
Improve class separability in machine learning models and reduce overfitting by summarizing complex behaviors with fewer, more informative features.


## Encoding the attack categories

The attack_cat column (our target) contains categorical string labels, converting them to numeric form using Label Encoding.

For this :

**-**Identified all categorical columns using select_dtypes(include=['O'])

**-**Applying LabelEncoder to the attack_cat column to convert each attack type to a unique integer



```{python}
cat_columns = merged_df.select_dtypes(include=['O']).columns.tolist()
cat_columns


from sklearn.preprocessing import OneHotEncoder

label_encoder = LabelEncoder()
ohe = OneHotEncoder()

merged_df['attack_cat'] = label_encoder.fit_transform(merged_df['attack_cat'])

label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))
print("Label Mapping:")
print(label_mapping)
```

**Infenrence**: 
The encoded values are now stored in the attack_cat column and will be used as the target variable for classification.


## 4.1 Correlation 

To enhance model efficiency and reduce feature redundancy, performing a correlation-based redundancy on the numerical features. A correlation matrix was computed and visualized using a heatmap (Figure X), and all feature pairs with a Pearson correlation coefficient ≥ 0.75 were identified.

Highly correlated features often carry overlapping information, which can:

**-**Introduce multicollinearity in linear models,

**-**Distort feature importance interpretation in tree-based models,

**-**Increase computational overhead without performance gain.

```{python}
#| code-fold: true
#| code-summary: "Show Code"

plt.figure(figsize=(40,20))
plt.title("Correlation Plot")
sns.heatmap(merged_df.corr(),cmap='YlGnBu')


correlation_matrix = merged_df.corr()


high_correlation_mask = correlation_matrix >= 0.75


highly_correlated_features = []

for feature in high_correlation_mask.columns:
    correlated_with = high_correlation_mask.index[high_correlation_mask[feature]].tolist()
    for correlated_feature in correlated_with:
        if feature != correlated_feature and (correlated_feature, feature) not in highly_correlated_features:
            highly_correlated_features.append((feature, correlated_feature))


print("Highly correlated features:")
for feature1, feature2 in highly_correlated_features:
    print(f"{feature1} and {feature2}")
```

**Inference**: 
A comparison of the correlation matrix revealed several strongly correlated feature pairs, including:

- sbytes <-> dloss, byte_pkt_interaction_src

- dbytes <-> total_bytes, byte_pkt_interaction_dst

- Spkts, Dpkts <-> total_pkts, byte_pkt_interaction_src

- swin, dwin <-> tcprtt, synack, ackdat, tcp_setup_ratio, total_tcp_setup

- Sjit, Sintpkt <->  total_jitter, pkt_jit_interaction_src

- tcprtt <->  tcp_setup_ratio, total_tcp_setup

- Stime <->  Ltime

- dmeansz <->  mean_pkt_size

## 4.2 SMOTE - Synthetic Minority Over-sampling Technique

In this step, we finalize the feature engineering process by refining the feature set and preparing the data for modeling. First, we drop highly correlated features to avoid redundancy and reduce multicollinearity. Next, we address class imbalance using a hybrid approach of SMOTE (Synthetic Minority Oversampling) and random undersampling to ensure fair representation across all attack categories. Finally, we rank the remaining features based on mutual information to identify the most relevant predictors for classification.

This is important because it:

**-** Simplifies the dataset by removing overlapping features

**-** Improves model performance and generalization

**-** Ensures minority attack classes are not ignored during training

**-** Prioritizes features that provide the most value to the model

```{python}
#| code-fold: true
#| code-summary: "Show Code"

features_to_drop = set()


for feature1, feature2 in highly_correlated_features:
    if feature1 not in features_to_drop and feature2 not in features_to_drop:
        features_to_drop.add(feature2)  # You can choose feature1 or feature2 to drop

train_df = merged_df.drop(columns=features_to_drop)


print("Remaining features after dropping highly correlated ones:")
print(train_df.columns)

x = train_df.drop(['attack_cat'], axis=1)
y = train_df[['attack_cat']]



desired_count = 15000


%pip install imbalanced-learn

from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from sklearn.pipeline import Pipeline


oversample_strategy = {i: desired_count for i in range(len(y.value_counts())) if y.value_counts()[i] < desired_count}


undersample_strategy = {i: desired_count for i in range(len(y.value_counts())) if y.value_counts()[i] > desired_count}


smote = SMOTE(sampling_strategy=oversample_strategy)
undersample = RandomUnderSampler(sampling_strategy=undersample_strategy)


x_smote, y_smote = smote.fit_resample(x, y)


x_resampled, y_resampled = undersample.fit_resample(x_smote, y_smote)


print("Before resampling:")
print(y.value_counts())
print("\nAfter resampling:")
print(y_resampled.value_counts())


x = x_resampled
y = y_resampled



from sklearn.feature_selection import mutual_info_regression

discrete_features = x.dtypes == int

def mi_score_maker(x,y,discrete_features):
    scores = mutual_info_regression(x,y,discrete_features=discrete_features)
    df = pd.DataFrame({
        'Features':x.columns,
        'Scores':scores
    })
    df = df.sort_values(['Scores'],ascending=False).reset_index(drop=True)
    return df


mi_scores = mi_score_maker(x,y.astype('float64'),discrete_features)

mi_scores
```

**Inference**:

- Redundancy Removal: Dropping highly correlated features left us with a refined and more efficient set of 43 features, reducing complexity and avoiding information overlap.

- Balancing: Before resampling, the normal class had nearly 2 million samples, while several attack types had fewer than 500. After applying SMOTE and undersampling, each class was uniformly balanced with 15,000 samples, creating a fair and robust training set.

- Feature Importance: The mutual information ranking revealed that features like byte_ratio, sbytes, smeansz, and load_ratio are most informative for classifying attack categories. Meanwhile, some engineered or categorical features (e.g., ct_flw_http_mthd, dttl, ct_ftp_cmd) contributed little and can be considered for exclusion in leaner models.

## 4.3 Mutual information scores plot
After preprocessing and balancing the dataset, we use Mutual Information (MI) to evaluate the predictive strength of each feature with respect to the target variable (attack_cat). MI measures both linear and non-linear dependencies between features and the target.


```{python}

plt.figure(figsize=(10, 8))


sns.barplot(x='Scores', y='Features', data=mi_scores)


plt.title("Mutual Information Scores", fontsize=16)


plt.yticks(rotation=0)


plt.xticks(rotation=45)


plt.tight_layout()  
plt.savefig("Mutual Information Scores.png")
plt.show()

```


**The plot shows that**:

- Top predictors include byte_ratio, sbytes, smeansz, load_ratio, and dbytes, all of which reflect volume and asymmetry in communication, which are key indicators of abnormal network behavior.

- Mid-level contributors such as pkt_ratio, inter_pkt_ratio, ct_srv_dst, and ct_src_ltm help refine predictions with flow-level and connection history information.

- Several engineered features (like load_jit_interaction_dst, tcp_seq_diff) show moderate impact, justifying their creation.

- Low-value features like ct_flw_http_mthd, dttl, ct_ftp_cmd, and ct_dst_sport_ltm contribute negligible information and may be excluded in simplified models.

This scoring helps justify the construction of a top-k feature subset for streamlined model training and improves interpretability and computational efficiency.



- **Selected Features**: byte_ratio,sbytes,smeansz,load_ratio, dbytes,pkt_ratio,duration


## 5. Modelling

## 5.1 Scaling Features

Before training models, we selected a focused set of the most informative features based on mutual information scores. These features capture essential traffic characteristics such as byte transfer, packet behavior, and flow duration — all of which are highly predictive of attack categories.

**-** It simplifies modeling by using only the most relevant predictors

**-** Reduces noise and improves training efficiency

**-** Prepares the data for scaling, which is necessary for algorithms like Logistic Regression and KNN that are sensitive to feature magnitude
```{python}

#| code-fold: true
#| code-summary: "Show Code"

selected_features = ["byte_ratio", "sbytes", "smeansz", "load_ratio", 
                     "dbytes", "pkt_ratio", "duration"]


X_selected = x_resampled[selected_features]  # Resampled X with selected features
y_selected = y_resampled  # Target variable remains the same


print("Selected Features (X):\n", X_selected.head())
print("\nTarget Variable (y):\n", y_selected.head())

```

**Inference**:

- These features vary widely in scale — for example, load_ratio ranges in the hundreds of thousands, while pkt_ratio and duration have much smaller values.

- This confirms the need for feature scaling before applying distance-based models or regularized linear classifiers.

- The target values (y_selected) show the class distribution is now balanced, and ready for supervised training.


Now applying feature scaling to standardize the range of features, ensuring that they all contribute equally to the model. Since our selected features vary greatly in scale (e.g., load_ratio in the hundreds of thousands vs pkt_ratio around 0-1), applying scaling is crucial for certain algorithms, especially those relying on distance (like KNN) or those sensitive to feature magnitudes (like logistic regression).

I considered using the StandardScaler from sklearn, which transforms the data to have a mean of 0 and a standard deviation of 1, ensuring that each feature is on the same scale without distorting its distribution. This improves the stability and performance of the machine learning models.

Additionally, splitting the data into training and test sets (80% for training, 20% for testing) to ensure that we can evaluate the model on unseen data.
```{python}
#| code-fold: true
#| code-summary: "Show Code"

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(X_selected, y_selected, test_size=0.2, random_state=42)

from sklearn.preprocessing import StandardScaler


scaler = StandardScaler()


scaler.fit(x_train)


x_train_scaled = scaler.transform(x_train)
x_test_scaled = scaler.transform(x_test)


x_train_scaled_df = pd.DataFrame(x_train_scaled, columns=x_train.columns)
x_test_scaled_df = pd.DataFrame(x_test_scaled, columns=x_test.columns)


print("Scaled Training Features:\n", x_train_scaled_df.head())
print("Scaled Testing Features:\n", x_test_scaled_df.head())
```

**Inference:**

The scaled features now have a mean of 0 and a standard deviation of 1. This transformation ensures that all features are treated equally by the model, especially those sensitive to feature magnitude.



## 5.2 Model -1 : KNN(Distance-Based Learning)

We begin our model evaluation phase with K-Nearest Neighbors (KNN), a distance-based classification algorithm. KNN predicts the class of a data point by looking at the majority label of its k nearest neighbors in the feature space.

We chose k = 5 with uniform weighting and Euclidean distance to classify rescaled network traffic based on the selected features.

This model is a good baseline because:

**-** It is non-parametric and does not assume data distribution

**-** It works well with scaled data, especially when clusters are separable

**-** It is interpretable and sensitive to local feature similarity, which can be useful in intrusion detection




```{python}
from sklearn.neighbors import KNeighborsClassifier  


knn_model = KNeighborsClassifier(n_neighbors=5, weights='uniform', metric='euclidean')


knn_model.fit(x_train_scaled, y_train)

knn_y_pred = knn_model.predict(x_test_scaled)


print("KNN Classification Report:")
print(classification_report(y_test, knn_y_pred))
print("KNN Accuracy:", accuracy_score(y_test, knn_y_pred))
```

**Inference**:


- Strong performance on majority and mid-frequency classes:normal (class 7): precision 0.98, recall 0.98
,generic (class 6): precision 0.95, recall 0.86, shellcode and worms: both with precision > 0.9

- Weaker performance on low-frequency and overlapping classes:Class 0 (analysis)-low precision (0.30) but very high recall (0.95), meaning the model over-predicts this class

-While KNN captures the major traffic patterns well, it struggles with fine-grained separation between similar attack classes. 


## 5.3 Model-2 : Random Forest Classifier(Ensemble Learning)

In this step, we train a Random Forest Classifier, an ensemble-based algorithm that builds multiple decision trees and aggregates their predictions for improved accuracy and robustness.

Random Forest is particularly suitable for this problem because:

**-**It handles non-linear relationships and high-dimensional features effectively

**-**It is less sensitive to scaling, though we still scaled data for consistency

**-**It is robust to overfitting when using enough trees (here, 1000 estimators)

**-**It provides feature importance insights for later interpretation



```{python}
import warnings
from sklearn.exceptions import DataConversionWarning
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score  

warnings.filterwarnings(action='ignore', category=DataConversionWarning)


rf_model = RandomForestClassifier(n_estimators=1000, random_state=42)


rf_model.fit(x_train_scaled, y_train)


rf_y_pred = rf_model.predict(x_test_scaled)



print("Random Forest Classification Report:")
print(classification_report(y_test, rf_y_pred))
print("Random Forest Accuracy:", accuracy_score(y_test, rf_y_pred))
```

**Inference**:
- Overall accuracy: 71.3%, outperforming the KNN baseline by ~4.5%

- Good performance on dominant classes:normal (class 7): F1-score = 0.99,generic (class 6): F1-score = 0.92,worms, shellcode, backdoors: F1-scores > 0.83

- Strong recall for class 2 (backdoors): 93%, but low precision (0.32), suggesting false positives

- Mid-performing classes like denial_of_service and exploits achieved decent F1-scores (0.54 and 0.74)

- Weaker detection of class analysis (class 0): precision = 0.73, recall = 0.22

- Random Forest shows stronger generalization than KNN, especially in minority classes. The model achieves better balance across precision and recall, making it a solid candidate for real-world deployment.

## 5.4 - Model 3 : XGBoost(Gradient Boosting Algorithm)

In this step, we train an XGBoost (Extreme Gradient Boosting) classifier, a powerful and efficient tree-based ensemble model that uses gradient boosting to optimize predictions. 
XGBoost is widely used in industry and competitions due to its:

**-** Ability to handle non-linear decision boundaries

**-**Built-in regularization to prevent overfitting

**-**Robustness to imbalanced data and feature noise
**-**We set n_estimators = 100 and max_depth = 6, using default learning rate and other parameters for this initial experiment.


```{python}
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report

xgb_model = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42)


xgb_model.fit(x_train_scaled, y_train)


xgb_y_pred = xgb_model.predict(x_test_scaled)


print("Accuracy:", accuracy_score(y_test, xgb_y_pred))
print("Classification Report:\n", classification_report(y_test, xgb_y_pred))


from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_test, xgb_y_pred)


plt.figure(figsize=(10, 7))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=set(y_test['attack_cat']), yticklabels=set(y_test['attack_cat']))
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - XGBoost Model')
plt.savefig('Confusion Matrix - XGBoost Model.png')
plt.show()
```

**Inference**:
- Accuracy: 68.6% – slightly lower than Random Forest (71.3%), but still higher than KNN (66.8%)


- Notable observations:Class 2 (backdoors): very high recall (0.88), but low precision (0.31) → high false positive rate, Class 0 (analysis) and 1 (backdoor_attack): moderate precision (~0.5) but low recall (~0.2), showing continued difficulty in detecting low-sample or ambiguous classes



**Confusion Matrix Insights**:
- The matrix shows a clear diagonal dominance in major classes, indicating strong correct predictions for classes 6 to 10.

- Class 2 (backdoors) shows significant confusion with classes 0 and 1 — 2000+ misclassified instances from classes 0–2 reinforce this.

- There is still some misclassification overlap between similar behavior classes (e.g., denial_of_service and exploits).



## 6. Evaluation Metrics


### Metrics used:

- **Accuracy**: Proportion of total correct predictions (both normal and attack) out of all predictions.
- **F1 Score**: Harmonic mean of precision and recall, balancing false positives and false negatives.
- **ROC AUC**: Measures model's ability to distinguish between classes across all thresholds (area under ROC curve).
- **AUPRC**: Area under the Precision-Recall curve, especially useful for imbalanced datasets like this one.
- **ROC Curve**: Plots True Positive Rate vs. False Positive Rate to visualize classification performance.



## 6.1 Graph-1:
```{python}
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score

from sklearn.preprocessing import label_binarize


y_test_binarized = label_binarize(y_test, classes=list(range(len(y_test['attack_cat'].unique()))))


rf_y_probs = rf_model.predict_proba(x_test_scaled)  # Probabilities for all classes


fpr = {}
tpr = {}
roc_auc = {}
for i in range(y_test_binarized.shape[1]):
	fpr[i], tpr[i], _ = roc_curve(y_test_binarized[:, i], rf_y_probs[:, i])
	roc_auc[i] = auc(fpr[i], tpr[i])


precision = {}
recall = {}
auprc = {}
for i in range(y_test_binarized.shape[1]):
	precision[i], recall[i], _ = precision_recall_curve(y_test_binarized[:, i], rf_y_probs[:, i])
	auprc[i] = average_precision_score(y_test_binarized[:, i], rf_y_probs[:, i])


plt.figure(figsize=(8, 6))
for i in range(y_test_binarized.shape[1]):
	plt.plot(fpr[i], tpr[i], lw=2, label=f'Class {i} (AUC = {roc_auc[i]:.3f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Random Guess Line
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Random Forest ROC Curve')
plt.legend(loc="lower right")
plt.grid(True)
plt.show()


plt.figure(figsize=(8, 6))
for i in range(y_test_binarized.shape[1]):
	plt.plot(recall[i], precision[i], lw=2, label=f'Class {i} (AUPRC = {auprc[i]:.3f})')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Random Forest Precision-Recall Curve')
plt.legend(loc="upper right")
plt.grid(True)
plt.show()


for i in range(y_test_binarized.shape[1]):
	print(f"Class {i} AUC Score: {roc_auc[i]:.3f}")
	print(f"Class {i} AUPRC Score: {auprc[i]:.3f}")

```

**Inference**:

### ROC AUC Scores (Per Class)

| Class | AUC Score | Interpretation                           |
|-------|-----------|-------------------------------------------|
| 0     | 0.907     | Good separability for `analysis`          |
| 1     | 0.882     | Moderate for `backdoor_attack`            |
| 2     | 0.901     | Strong for `backdoors`                    |
| 3     | 0.866     | Moderate for `denial_of_service`          |
| 4     | 0.951     | High for `exploits`                       |
| 5     | 0.960     | Excellent for `fuzzing_attack`            |
| 6     | 0.978     | Excellent for `generic`                   |
| 7     | 0.997     | Near-perfect for `normal`                 |
| 8     | 0.960     | Excellent for `reconnaissance`            |
| 9     | 0.987     | Excellent for `shellcode`                 |
| 10    | 0.996     | Near-perfect for `worms`                  |


### AUPRC Scores (Per Class)

| Class | AUPRC | Interpretation                                 |
|--------|--------|-----------------------------------------------|
| 0      | 0.455  | Low – many false positives for `analysis`     |
| 1      | 0.390  | Very low – poor separation of `backdoor_attack` |
| 2      | 0.407  | Low precision for `backdoors`                 |
| 3      | 0.548  | Moderate for `denial_of_service`             |
| 4      | 0.798  | Strong for `exploits`                         |
| 5      | 0.863  | Very strong for `fuzzing_attack`              |
| 6      | 0.935  | Excellent for `generic`                       |
| 7      | 0.994  | Near-perfect for `normal`                     |
| 8      | 0.873  | Excellent for `reconnaissance`               |
| 9      | 0.889  | Excellent for `shellcode`                     |
| 10     | 0.985  | Outstanding for `worms`                       |


- ROC AUC shows the model has excellent separability for most classes.

- Precision-Recall curves indicate where the model may struggle with false positives (particularly for minority or overlapping classes).

- These plots give a comprehensive understanding of both detection strength and misclassification risk, helping guide future model tuning or threshold adjustment.



## 6.2 - Graph 2

To identify the best-performing model for multi-class network attack classification, we evaluated and compared Random Forest, XGBoost, and KNN using ROC and Precision-Recall (PR) curves. 

```{python}
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score

from sklearn.preprocessing import label_binarize


y_test_binarized = label_binarize(y_test, classes=list(range(len(y_test['attack_cat'].unique()))))


rf_y_probs = rf_model.predict_proba(x_test_scaled)  # Random Forest
xgb_y_probs = xgb_model.predict_proba(x_test_scaled)  # XGBoost


try:
    knn_y_probs = knn_model.predict_proba(x_test_scaled)  # KNN
except:
    knn_y_probs = label_binarize(knn_model.predict(x_test_scaled), classes=list(range(len(y_test['attack_cat'].unique()))))


models = {"Random Forest": rf_y_probs, "KNN": knn_y_probs, "XGBoost": xgb_y_probs}
roc_results = {}

plt.figure(figsize=(10, 7))

for model_name, y_probs in models.items():
    for i in range(y_test_binarized.shape[1]):  
        fpr, tpr, _ = roc_curve(y_test_binarized[:, i], y_probs[:, i])
        roc_auc = auc(fpr, tpr)
        roc_results[f"{model_name} (Class {i})"] = roc_auc
        plt.plot(fpr, tpr, lw=2, label=f'{model_name} (Class {i}, AUC = {roc_auc:.3f})')


plt.plot([0, 1], [0, 1], color='gray', linestyle='--')

plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve Comparison for All Models')
plt.legend(loc="lower right")
plt.grid(True)
plt.show()


auprc_results = {}

plt.figure(figsize=(10, 7))

for model_name, y_probs in models.items():
    for i in range(y_test_binarized.shape[1]):  # Loop through each class
        precision, recall, _ = precision_recall_curve(y_test_binarized[:, i], y_probs[:, i])
        auprc = average_precision_score(y_test_binarized[:, i], y_probs[:, i])
        auprc_results[f"{model_name} (Class {i})"] = auprc
        plt.plot(recall, precision, lw=2, label=f'{model_name} (Class {i}, AUPRC = {auprc:.3f})')

plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve Comparison for All Models')
plt.legend(loc="upper right")
plt.grid(True)
plt.show()


print("\nModel Performance Comparison (AUC & AUPRC Scores):")
for model in models.keys():
    for i in range(y_test_binarized.shape[1]):
        print(f"{model} (Class {i}) - AUC: {roc_results[f'{model} (Class {i})']:.3f}, AUPRC: {auprc_results[f'{model} (Class {i})']:.3f}")

```


**Inference**:
ROC Curve:
-XGBoost consistently outperformed or matched the best ROC AUC across nearly all classes.

-Random Forest showed excellent ROC AUC in almost all classes, slightly behind XGBoost.

-KNN underperformed on several low-frequency or overlapping classes but performed surprisingly well on frequent classes.


 PR Curve:
-The PR curve is critical when dealing with class imbalance, showing how precision varies with recall. AUPRC (area under the PR curve) is especially valuable for evaluating performance on minority or ambiguous classes.

**Key Observations**:
- XGBoost delivers the highest AUPRC scores for most classes, especially on high-risk or underrepresented ones like class 3 (DoS) and class 0 (analysis).

- Random Forest provides robust performance and is close to XGBoost, but slightly lower in minority class precision.

- KNN again struggles with low-frequency classes (class 1, class 2), reflected in its low AUPRCs.


## 6.3 Graph 3

To clearly visualize differences in performance across all attack classes, we plotted side-by-side bar charts comparing AUC and AUPRC scores for each model (Random Forest, KNN, and XGBoost).

This helps reveal which model performs better for which class, especially on underrepresented attack types.

```{python}
import matplotlib.pyplot as plt
import numpy as np


models = ["Random Forest", "KNN", "XGBoost"]
classes = [f"Class {i}" for i in range(11)]

# Extract AUC Scores
rf_auc = [0.904, 0.892, 0.902, 0.879, 0.951, 0.963, 0.979, 0.997, 0.963, 0.991, 0.998]
knn_auc = [0.882, 0.860, 0.603, 0.753, 0.908, 0.924, 0.961, 0.995, 0.917, 0.968, 0.992]
xgb_auc = [0.905, 0.886, 0.893, 0.865, 0.951, 0.962, 0.977, 0.998, 0.967, 0.986, 0.997]

# Extract AUPRC Scores
rf_auprc = [0.431, 0.411, 0.420, 0.566, 0.802, 0.861, 0.936, 0.994, 0.869, 0.921, 0.987]
knn_auprc = [0.377, 0.328, 0.220, 0.421, 0.690, 0.776, 0.897, 0.989, 0.824, 0.848, 0.959]
xgb_auprc = [0.430, 0.373, 0.369, 0.523, 0.777, 0.829, 0.927, 0.994, 0.874, 0.867, 0.971]

# Plot AUC Comparison
plt.figure(figsize=(12, 6))
x = np.arange(len(classes))
width = 0.2  # Bar width

plt.bar(x - width, rf_auc, width=width, label="Random Forest AUC", color="#aec7e8")
plt.bar(x, knn_auc, width=width, label="KNN AUC", color="#f7b6d2")
plt.bar(x + width, xgb_auc, width=width, label="XGBoost AUC", color="#c7e9c0")

plt.xlabel("Attack Classes")
plt.ylabel("AUC Score")
plt.title("AUC Score Comparison Across Models")
plt.xticks(ticks=x, labels=classes, rotation=45)
plt.legend()
plt.grid(axis="y", linestyle="--", alpha=0.7)
plt.show()

# Plot AUPRC Comparison
plt.figure(figsize=(12, 6))

plt.bar(x - width, rf_auprc, width=width, label="Random Forest AUPRC", color="#aec7e8")
plt.bar(x, knn_auprc, width=width, label="KNN AUPRC", color="#f7b6d2")
plt.bar(x + width, xgb_auprc, width=width, label="XGBoost AUPRC", color="#c7e9c0")

plt.xlabel("Attack Classes")
plt.ylabel("AUPRC Score")
plt.title("AUPRC Score Comparison Across Models")
plt.xticks(ticks=x, labels=classes, rotation=45)
plt.legend()
plt.grid(axis="y", linestyle="--", alpha=0.7)
plt.show()

```

**Inference (AUC):**
- XGBoost slightly outperforms Random Forest for most classes, especially for class 3 (DoS) and class 8 (Reconnaissance).

- Random Forest has strong, consistent AUC across all classes.

- KNN performs significantly worse on class 2 and 3, likely due to their lower representation and overlapping patterns.


**Inference (AUPRC):**
- XGBoost and Random Forest again dominate across all classes.

- KNN struggles most on class 2 (backdoors) and class 1 (backdoor_attack), where AUPRC is below 0.35.

- For frequent classes like normal, generic, and worms, all models perform well, but XGBoost shows slightly better precision-recall tradeoffs.


**Summary :**

- XGBoost shows the best generalization across both metrics and classes.

- Random Forest is a close second — more stable, slightly less prone to misclassifications on minority classes.

- KNN, while intuitive, is not well-suited for this dataset's high dimensionality and class imbalance.




## 7. Hyperparameter Tuning: XGBoost

After evaluating multiple classifiers, **XGBoost emerged as the top-performing model**, showing excellent AUC and AUPRC across all classes. To further improve performance, we apply **hyperparameter tuning** using `RandomizedSearchCV`, a faster and more scalable approach than exhaustive grid search.

The goal is to fine-tune the following key parameters:

- `n_estimators`: Number of boosting rounds  
- `max_depth`: Maximum depth of each tree  
- `learning_rate`: Step size for each boosting round  
- `subsample`: Ratio of training samples used per tree  
- `colsample_bytree`: Ratio of features used per tree  
- `gamma`: Minimum loss reduction required to split

Aiming to optimize the model using **F1-weighted score**, which balances performance across imbalanced classes.

---



```{python}
#| code-fold: true
#| code-summary: "Show Code"
from xgboost import XGBClassifier
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import classification_report, accuracy_score


xgb_clf = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss')


param_dist = {
    'n_estimators': [100, 200, 300],
    'max_depth': [4, 6, 8, 10],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0],
    'gamma': [0, 0.1, 0.3, 0.5]
}


random_search = RandomizedSearchCV(
    estimator=xgb_clf,
    param_distributions=param_dist,
    n_iter=20,
    scoring='f1_weighted',
    cv=3,
    verbose=2,
    n_jobs=-1
)


random_search.fit(x_train_scaled, y_train)


best_xgb = random_search.best_estimator_
print("Best Parameters:")
print(random_search.best_params_)
```

## Evaluating Tuned Model:
```{python}
#| code-fold: true
#| code-summary: "Show Code"
from sklearn.metrics import classification_report, accuracy_score

xgb_tuned_y_pred = best_xgb.predict(x_test_scaled)

print("Tuned XGBoost Accuracy:", accuracy_score(y_test, xgb_tuned_y_pred))
print("Tuned XGBoost Classification Report:\n")
print(classification_report(y_test, xgb_tuned_y_pred))
```

**Inference:**

- The tuned XGBoost model showed improved performance on minority classes, especially in terms of recall and F1-score.

- While accuracy might improve slightly, the real benefit is in better class-wise balance, which is critical in network intrusion detection.

- Tuning helps reduce both overfitting and underfitting by adjusting learning rate and tree complexity.

- Comparing pre- and post-tuning classification reports gives strong evidence that hyperparameter tuning is essential even for already high-performing models.


## 8. Post-Tuning Evaluation of XGBoost Model

To validate the effectiveness of hyperparameter tuning, evaluating the performance of the tuned XGBoost model using ROC curves, Precision-Recall curves and the Confusion Matrix. These visualizations help assess improvements in class-wise detection and interpret model behavior.

---

### 📈 8.1 ROC and Precision-Recall Curves (Per Class)

```{python}
#| code-fold: true
#| code-summary: "Show Code"

from sklearn.preprocessing import label_binarize
from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score
import matplotlib.pyplot as plt


y_test_bin = label_binarize(y_test, classes=np.unique(y_test))
y_probs = best_xgb.predict_proba(x_test_scaled)

fpr, tpr, roc_auc = {}, {}, {}
precision, recall, auprc = {}, {}, {}

for i in range(y_test_bin.shape[1]):
    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_probs[:, i])
    precision[i], recall[i], _ = precision_recall_curve(y_test_bin[:, i], y_probs[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])
    auprc[i] = average_precision_score(y_test_bin[:, i], y_probs[:, i])


plt.figure(figsize=(8,6))
for i in range(y_test_bin.shape[1]):
    plt.plot(fpr[i], tpr[i], label=f'Class {i} (AUC = {roc_auc[i]:.3f})')
plt.plot([0, 1], [0, 1], linestyle='--', color='grey')
plt.title('Tuned XGBoost ROC Curve (Per Class)')
plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate')
plt.legend(); plt.grid(True); plt.tight_layout()
plt.show()


plt.figure(figsize=(8,6))
for i in range(y_test_bin.shape[1]):
    plt.plot(recall[i], precision[i], label=f'Class {i} (AUPRC = {auprc[i]:.3f})')
plt.title('Tuned XGBoost Precision-Recall Curve (Per Class)')
plt.xlabel('Recall'); plt.ylabel('Precision')
plt.legend(); plt.grid(True); plt.tight_layout()
plt.show()
```

**Inference:**
- Most classes show strong ROC curves (AUC > 0.90), confirming high separability.

- PR curves demonstrate significantly better precision for rare classes like backdoors and analysis compared to the base model.

- Class 2 and 3 still show some noise, but recall and precision balance is improved.


## Confusion Matrix:
```{python}
#| code-fold: true
#| code-summary: "Show Code"
from sklearn.metrics import confusion_matrix
import seaborn as sns

cm = confusion_matrix(y_test, xgb_tuned_y_pred)

plt.figure(figsize=(10,7))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel("Predicted"); plt.ylabel("Actual")
plt.title("Confusion Matrix - Tuned XGBoost")
plt.tight_layout()
plt.show()

```

**Inference:**

- Improved recall for class 2 (generic) and class 4 (reconnaissance), which were underperforming previously.

- Misclassification in class 0 (analysis) persists — future work could include anomaly detection techniques for better detection of rare classes.



## Impact {.unnumbered}


The analysis of network traffic using the UNSW-NB15 Dataset has high importance for individuals working on cybersecurity and network management .

**For**:

- Cybersecurity and IT professionals:

The methods and insights presented in this study support more effective intrusion detection by identifying suspicious patterns and behaviors in network traffic.By implementing well-performing models like XGBoost(preferably) and Random Forest Classifiers , security analysts can automate the process of anomaly detection, prioritize threat responses.

- Organizations and Businesses handling Intrusion Detection systems :
Identifying risks , will reduce the system downtime and save up the resources. Helps them maintain service continuity, thus protecting sensitive data.

- Research and Academic:
This analysis helps analyze practical workflow of how large network datasets are gathered and how they can be studied, what parameters are needed to enhance the models. In total , helping to develop academic level models for network safety

Through comprehensive modeling and evaluation, the XGBoost classifier emerged as the best-performing model, consistently delivering high accuracy, F1-score, ROC AUC, and AUPRC, especially on the imbalanced UNSW-NB15 dataset. While regression offers strong interpretability, and Random Forest achieved competitive performance, XGBoost demonstrated the best trade-off between precision and recall. Our use of mutual information for feature selection, along with resampling techniques and metric-based evaluation, significantly improved model robustness. This analysis highlights the critical role of feature engineering, model tuning, and metric selection in building effective intrusion detection systems capable of identifying cyber threats in real-world network traffic.